{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:47.343702Z",
     "start_time": "2025-05-14T15:44:47.322769Z"
    }
   },
   "source": [
    "import argparse\n",
    "import logging\n",
    "import pathlib\n",
    "import pprint\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import tqdm\n",
    "from torch.xpu import device_of\n",
    "\n",
    "import dataset\n",
    "import music_x_transformers\n",
    "import representation\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "import representation\n",
    "from music_x_transformers import MusicXTransformer\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:47.960680Z",
     "start_time": "2025-05-14T15:44:47.945054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "742555f5dcce5bec",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.007945Z",
     "start_time": "2025-05-14T15:44:47.992316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ],
   "id": "6fff3f3cb8ec6f80",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.039197Z",
     "start_time": "2025-05-14T15:44:48.007945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import another midi file and encode\n",
    "import representation\n",
    "import muspy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "encoding_path = \"../data/sod/processed/notes/encoding.json\"\n",
    "midi_in_path = \"../data/sod/SOD/Kunstderfuge/0/1.mid\"\n",
    "\n",
    "# Set the output base to your Desktop\n",
    "desktop = os.path.expanduser(\"~/Desktop\")\n",
    "out_base = os.path.join(desktop, \"1_out\")\n",
    "\n",
    "# Load encoding\n",
    "encoding = representation.load_encoding(encoding_path)"
   ],
   "id": "66ac939031ee43f2",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.547167Z",
     "start_time": "2025-05-14T15:44:48.091524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read MIDI as MusPy object\n",
    "music = muspy.read_midi(midi_in_path)\n",
    "\n",
    "# Fix resolution mismatch if needed\n",
    "expected_resolution = encoding[\"resolution\"]\n",
    "if music.resolution != expected_resolution:\n",
    "    music.adjust_resolution(expected_resolution)\n"
   ],
   "id": "796a41911e8be88a",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.639806Z",
     "start_time": "2025-05-14T15:44:48.578420Z"
    }
   },
   "cell_type": "code",
   "source": "codes = representation.encode(music, encoding)  # shape: (seq_len, 6)",
   "id": "f5ad8d9f1e6e1e86",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.687174Z",
     "start_time": "2025-05-14T15:44:48.671166Z"
    }
   },
   "cell_type": "code",
   "source": "codes.shape",
   "id": "f71034145dd0be01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14550, 6)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.738080Z",
     "start_time": "2025-05-14T15:44:48.722454Z"
    }
   },
   "cell_type": "code",
   "source": "entire_len = codes.shape[0]",
   "id": "b2c8f0345c2049ce",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.784959Z",
     "start_time": "2025-05-14T15:44:48.769332Z"
    }
   },
   "cell_type": "code",
   "source": "entire_len",
   "id": "970440d8121317b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14550"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.863477Z",
     "start_time": "2025-05-14T15:44:48.816598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "instruments = codes[:, 5]\n",
    "pitches = codes[:, 3]\n",
    "\n",
    "# Find min/max pitch per instrument\n",
    "pitch_ranges = {}\n",
    "for inst in np.unique(instruments):\n",
    "    inst_pitches = pitches[instruments == inst]\n",
    "    pitch_ranges[inst] = (inst_pitches.min(), inst_pitches.max())\n",
    "\n",
    "# For each event, sample a random pitch within the range of its instrument\n",
    "rand_pitches = np.zeros_like(pitches)\n",
    "for idx, inst in enumerate(instruments):\n",
    "    lo, hi = pitch_ranges[inst]\n",
    "    rand_pitches[idx] = np.random.randint(lo, hi + 1)  # inclusive"
   ],
   "id": "462aadaecdebc590",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.910740Z",
     "start_time": "2025-05-14T15:44:48.895111Z"
    }
   },
   "cell_type": "code",
   "source": "rand_pitches.min()",
   "id": "1a39c46e4b592537",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:48.973246Z",
     "start_time": "2025-05-14T15:44:48.957618Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "98994b30e4eea2f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:49.020511Z",
     "start_time": "2025-05-14T15:44:49.004883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exclude zeros\n",
    "nonzero_pitches = pitches[pitches != 0]\n",
    "min_pitch = nonzero_pitches.min()\n",
    "print(min_pitch)"
   ],
   "id": "52191981fd23e3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:49.077557Z",
     "start_time": "2025-05-14T15:44:49.061930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "orig_durations = codes[:, 4]\n",
    "log_durs = np.log(orig_durations + 1e-8)  # avoid log(0)\n",
    "mean_log_dur = log_durs.mean()\n",
    "std_log_dur = log_durs.std()\n",
    "nonzero_durations = orig_durations[orig_durations > 0]\n",
    "min_dur = nonzero_durations.min()\n",
    "max_dur = nonzero_durations.max()\n",
    "\n",
    "mean_log_dur_tensor = torch.tensor(mean_log_dur)\n",
    "\n",
    "min_dur_tensor = torch.tensor(min_dur, dtype=mean_log_dur_tensor.dtype)\n",
    "max_dur_tensor = torch.tensor(max_dur, dtype=mean_log_dur_tensor.dtype)\n",
    "\n",
    "min_dur_tensor = min_dur_tensor.to(device)\n",
    "max_dur_tensor = max_dur_tensor.to(device)\n",
    "\n",
    "max_seq_len = 256\n",
    "\n",
    "\n",
    "rand_log_dur = np.random.normal(mean_log_dur, std_log_dur, size=orig_durations.shape)\n",
    "rand_durations = np.exp(rand_log_dur)"
   ],
   "id": "db59a3577ebc5dfe",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:49.124821Z",
     "start_time": "2025-05-14T15:44:49.109194Z"
    }
   },
   "cell_type": "code",
   "source": "rand_durations.max()",
   "id": "25faa1149f2e35f1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224.43119936902576"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:44:49.172083Z",
     "start_time": "2025-05-14T15:44:49.156454Z"
    }
   },
   "cell_type": "code",
   "source": "max_dur",
   "id": "a66986e550b159d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:45.346324Z",
     "start_time": "2025-05-14T15:45:45.330697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Recreate model with training hyperparameters\n",
    "model = MusicXTransformer(\n",
    "    dim=64,\n",
    "    encoding=encoding,\n",
    "    depth=3,\n",
    "    heads=4,\n",
    "    max_seq_len=256,\n",
    "    max_beat=64,\n",
    "    rotary_pos_emb=False,\n",
    "    use_abs_pos_emb=True,\n",
    "    emb_dropout=0.2,\n",
    "    attn_dropout=0.2,\n",
    "    ff_dropout=0.2,\n",
    ")"
   ],
   "id": "b6898cff8514391e",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:47.146046Z",
     "start_time": "2025-05-14T15:45:47.131191Z"
    }
   },
   "cell_type": "code",
   "source": "model = model.to(device)",
   "id": "48ac100c677815a0",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:48.911348Z",
     "start_time": "2025-05-14T15:45:48.879713Z"
    }
   },
   "cell_type": "code",
   "source": "model.load_state_dict(torch.load(\"../exp/sod/ape/checkpoints/model_200.pt\", map_location=device))",
   "id": "6a5b2b9b2c7be49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:58.454475Z",
     "start_time": "2025-05-14T15:45:58.438848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class SuperModel(nn.Module):\n",
    "#     def __init__(self, , max_seq_len=32):\n",
    "#         super(SuperModel, self).__init__()\n",
    "#\n",
    "#         self.pit = nn.Parameter(torch.tensor(rand_pitches, dtype=torch.float32).reshape(1, -1, 1))  # shape (1, 11302, 1)\n",
    "#         self.dur = nn.Parameter(torch.tensor(rand_durations, dtype=torch.float32).reshape(1, -1, 1))\n",
    "#\n",
    "#         self.lower_bound = torch.max(torch.exp(mean_log_dur)/8, min_dur_tensor).item()\n",
    "#         self.upper_bound = torch.min(torch.exp(mean_log_dur)*128, torch.tensor(32).to(device)).item()\n",
    "#\n",
    "#     def forward(self, ix):  #pitches must just have 1 batch. This is to just work for 1 batch, to overfit is good.\n",
    "#         quantized_pit = torch.round(self.pit).clamp(21, 100)\n",
    "#         clamped_dur = self.dur.clamp(self.lower_bound, self.upper_bound)\n",
    "#\n",
    "#\n",
    "#\n",
    "#         x = model(seq)\n",
    "#\n"
   ],
   "id": "3fc2d20ab98cb6c1",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:45:59.017864Z",
     "start_time": "2025-05-14T15:45:59.001854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SuperModel(nn.Module):\n",
    "    def __init__(self, mean_log_dur_tensor, min_dur_tensor, device, max_seq_len=1024):\n",
    "        super(SuperModel, self).__init__()\n",
    "        self.pit = nn.Parameter(torch.tensor(rand_pitches, dtype=torch.float32, device=device).reshape(1, -1, 1))\n",
    "        self.dur = nn.Parameter(torch.tensor(rand_durations, dtype=torch.float32, device=device).reshape(1, -1, 1))\n",
    "        self.lower_bound = torch.max(torch.exp(mean_log_dur_tensor)/8, min_dur_tensor).item()\n",
    "        self.upper_bound = torch.min(torch.exp(mean_log_dur_tensor)*128, torch.tensor(32, device=device)).item()\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, codes, ix):\n",
    "        # codes: numpy array (N, 6)\n",
    "        # ix: int (start index)\n",
    "        seq = torch.tensor(codes[ix:ix+self.max_seq_len], dtype=torch.long, device=self.pit.device)\n",
    "\n",
    "        quantized_pit = torch.round(self.pit).clamp(21, 100)  # (1, N, 1)\n",
    "        clamped_dur = self.dur.clamp(self.lower_bound, self.upper_bound)  # (1, N, 1)\n",
    "\n",
    "        # Extract pitch and duration chunk\n",
    "        pitch_chunk = quantized_pit[0, ix:ix+self.max_seq_len, 0]\n",
    "        dur_chunk = clamped_dur[0, ix:ix+self.max_seq_len, 0]\n",
    "\n",
    "        # Round duration, set any positive value <1 to 1\n",
    "        rounded_dur = torch.round(dur_chunk)\n",
    "        rounded_dur = torch.where((rounded_dur < 1) & (dur_chunk > 0), torch.ones_like(rounded_dur), rounded_dur)\n",
    "\n",
    "        # Replace pitch and duration fields\n",
    "        seq[:, 3] = pitch_chunk.long()\n",
    "        seq[:, 4] = rounded_dur.long()\n",
    "\n",
    "         # Post-process the beat field\n",
    "        orig_first = seq[0, 1].item()\n",
    "        seq[:, 1] = seq[:, 1] - orig_first + 1\n",
    "        seq[:, 1] = torch.clamp(seq[:, 1], min=0)\n",
    "\n",
    "        seq = seq.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Pass to your model\n",
    "        x = model(seq)\n",
    "\n",
    "        return x"
   ],
   "id": "e096e494e60beab6",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:46:00.510254Z",
     "start_time": "2025-05-14T15:46:00.494627Z"
    }
   },
   "cell_type": "code",
   "source": "seq = torch.tensor(codes[10000:10000+32], dtype=torch.long)\n",
   "id": "d715cbf98b5d3b9e",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:46:00.795012Z",
     "start_time": "2025-05-14T15:46:00.779385Z"
    }
   },
   "cell_type": "code",
   "source": "seq.shape",
   "id": "f8f9674c2dbc10d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:46:01.265791Z",
     "start_time": "2025-05-14T15:46:01.250165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "supermodel = SuperModel(\n",
    "\n",
    "\n",
    "    mean_log_dur_tensor,\n",
    "    min_dur_tensor,\n",
    "    device,\n",
    "    max_seq_len\n",
    ")"
   ],
   "id": "5179311a03a0d9ce",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:46:02.434389Z",
     "start_time": "2025-05-14T15:46:02.418761Z"
    }
   },
   "cell_type": "code",
   "source": " codes[0:32]",
   "id": "e52534858eb4892c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  1],\n",
       "       [ 1,  0,  0,  0,  0,  6],\n",
       "       [ 1,  0,  0,  0,  0, 25],\n",
       "       [ 1,  0,  0,  0,  0, 26],\n",
       "       [ 1,  0,  0,  0,  0, 27],\n",
       "       [ 1,  0,  0,  0,  0, 28],\n",
       "       [ 1,  0,  0,  0,  0, 29],\n",
       "       [ 1,  0,  0,  0,  0, 31],\n",
       "       [ 1,  0,  0,  0,  0, 35],\n",
       "       [ 1,  0,  0,  0,  0, 38],\n",
       "       [ 1,  0,  0,  0,  0, 45],\n",
       "       [ 1,  0,  0,  0,  0, 46],\n",
       "       [ 1,  0,  0,  0,  0, 47],\n",
       "       [ 1,  0,  0,  0,  0, 48],\n",
       "       [ 1,  0,  0,  0,  0, 50],\n",
       "       [ 2,  0,  0,  0,  0,  0],\n",
       "       [ 3,  7,  1, 50, 12, 31],\n",
       "       [ 3,  7,  1, 50, 25, 47],\n",
       "       [ 3,  7,  1, 57, 12, 31],\n",
       "       [ 3,  7,  1, 57, 25, 47],\n",
       "       [ 3,  7,  1, 58, 23, 48],\n",
       "       [ 3,  7,  1, 62, 23, 48],\n",
       "       [ 3,  7,  1, 69, 23, 46],\n",
       "       [ 3,  8,  1, 31, 12, 31],\n",
       "       [ 3,  8,  1, 43, 12, 31],\n",
       "       [ 3,  8,  1, 62, 12, 31],\n",
       "       [ 3,  8,  1, 69, 12, 31],\n",
       "       [ 3,  9,  1, 74, 18, 31],\n",
       "       [ 3,  9,  1, 81, 18, 31],\n",
       "       [ 3, 10,  1, 81,  6,  6],\n",
       "       [ 3, 10,  7, 74,  6,  6]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:46:03.535701Z",
     "start_time": "2025-05-14T15:46:03.517068Z"
    }
   },
   "cell_type": "code",
   "source": "codes.shape",
   "id": "efeaeaaa0047ce75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14550, 6)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:46:04.911999Z",
     "start_time": "2025-05-14T15:46:04.880746Z"
    }
   },
   "cell_type": "code",
   "source": "supermodel(codes,14000)",
   "id": "efd9fbd6f21f0a77",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:46:51.352082Z",
     "start_time": "2025-05-14T15:46:16.799035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(supermodel.parameters(), lr=1e-3)\n",
    "\n",
    "loss_total = 0.0\n",
    "\n",
    "supermodel.train()  # Set model to training mode\n",
    "for ix in range(1, 1400):  # inclusive of 14000\n",
    "    loss = supermodel(codes, ix)\n",
    "    # If your loss is a tensor, make sure to use .item() or .sum() as appropriate\n",
    "    loss_total += loss if isinstance(loss, float) else loss.sum()\n",
    "\n",
    "# If you want to backpropagate:\n",
    "optimizer.zero_grad()\n",
    "loss_total.backward()\n",
    "optimizer.step()"
   ],
   "id": "bfe9ffb0a26006ac",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:50:51.202140Z",
     "start_time": "2025-05-14T15:48:39.246446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_steps = 2000\n",
    "\n",
    "for step in range(num_steps):\n",
    "    supermodel.train()\n",
    "    loss_total = 0.0\n",
    "\n",
    "    for ix in range(1, 1400):  # You probably meant 14000 for full range\n",
    "        loss = supermodel(codes, ix)\n",
    "        loss_total += loss if isinstance(loss, float) else loss.sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Step {step+1}/{num_steps} - Loss: {loss_total.item() if hasattr(loss_total, 'item') else loss_total}\")"
   ],
   "id": "ca1ec15ee6e94484",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/2000 - Loss: 27.806901931762695\n",
      "Step 2/2000 - Loss: 27.38190269470215\n",
      "Step 3/2000 - Loss: 27.271635055541992\n",
      "Step 4/2000 - Loss: 28.607372283935547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[96], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m loss_total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ix \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1400\u001B[39m):  \u001B[38;5;66;03m# You probably meant 14000 for full range\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43msupermodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     loss_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(loss, \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m loss\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[87], line 41\u001B[0m, in \u001B[0;36mSuperModel.forward\u001B[1;34m(self, codes, ix)\u001B[0m\n\u001B[0;32m     35\u001B[0m seq \u001B[38;5;241m=\u001B[39m seq\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Pass to your model\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Users\\ly\\PycharmProjects\\mtmt723\\mmt\\music_x_transformers.py:560\u001B[0m, in \u001B[0;36mMusicXTransformer.forward\u001B[1;34m(self, seq, mask, **kwargs)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, seq, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 560\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(seq, mask\u001B[38;5;241m=\u001B[39mmask, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Users\\ly\\PycharmProjects\\mtmt723\\mmt\\music_x_transformers.py:523\u001B[0m, in \u001B[0;36mMusicAutoregressiveWrapper.forward\u001B[1;34m(self, x, return_list, **kwargs)\u001B[0m\n\u001B[0;32m    520\u001B[0m     mask \u001B[38;5;241m=\u001B[39m mask[:, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    521\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmask\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m mask\n\u001B[1;32m--> 523\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnet(xi, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    525\u001B[0m losses \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    526\u001B[0m target_one \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones_like(out)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Users\\ly\\PycharmProjects\\mtmt723\\mmt\\music_x_transformers.py:220\u001B[0m, in \u001B[0;36mMusicTransformerWrapper.forward\u001B[1;34m(self, x, return_embeddings, mask, return_mems, return_attn, mems, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m     mems_l, mems_r \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    215\u001B[0m         mems[: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshift_mem_down],\n\u001B[0;32m    216\u001B[0m         mems[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshift_mem_down:],\n\u001B[0;32m    217\u001B[0m     )\n\u001B[0;32m    218\u001B[0m     mems \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39mmems_r, \u001B[38;5;241m*\u001B[39mmems_l]\n\u001B[1;32m--> 220\u001B[0m x4, intermediates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_layers(\n\u001B[0;32m    221\u001B[0m     x3, mask\u001B[38;5;241m=\u001B[39mmask, mems\u001B[38;5;241m=\u001B[39mmems, return_hiddens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    222\u001B[0m )\n\u001B[0;32m    223\u001B[0m x4 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x4)\n\u001B[0;32m    225\u001B[0m out_one \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_one(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeedforward(x4))  \u001B[38;5;66;03m# (B, S+num_mem, 1)\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\x_transformers\\x_transformers.py:965\u001B[0m, in \u001B[0;36mAttentionLayers.forward\u001B[1;34m(self, x, context, mask, context_mask, attn_mask, mems, return_hiddens)\u001B[0m\n\u001B[0;32m    963\u001B[0m     out, inter \u001B[38;5;241m=\u001B[39m block(x, context \u001B[38;5;241m=\u001B[39m context, mask \u001B[38;5;241m=\u001B[39m mask, context_mask \u001B[38;5;241m=\u001B[39m context_mask, prev_attn \u001B[38;5;241m=\u001B[39m prev_cross_attn)\n\u001B[0;32m    964\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m layer_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 965\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exists(post_branch_norm):\n\u001B[0;32m    968\u001B[0m     out \u001B[38;5;241m=\u001B[39m post_branch_norm(out)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\x_transformers\\x_transformers.py:501\u001B[0m, in \u001B[0;36mFeedForward.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 240\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:05:06.910784Z",
     "start_time": "2025-05-14T16:05:06.895166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "supermodel = SuperModel(\n",
    "\n",
    "\n",
    "    mean_log_dur_tensor,\n",
    "    min_dur_tensor,\n",
    "    device,\n",
    "    max_seq_len\n",
    ")"
   ],
   "id": "e822623dfff571e8",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:01:49.072783Z",
     "start_time": "2025-05-14T15:51:20.259725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_steps = 500\n",
    "\n",
    "for step in range(num_steps):\n",
    "    supermodel.train()\n",
    "    loss_total = 0.0\n",
    "\n",
    "    for ix in range(1, 700):  # You probably meant 14000 for full range\n",
    "        loss = supermodel(codes, ix)\n",
    "        loss_total += loss if isinstance(loss, float) else loss.sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Step {step+1}/{num_steps} - Loss: {loss_total.item() if hasattr(loss_total, 'item') else loss_total}\")"
   ],
   "id": "5af92f5052d5d1d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/500 - Loss: 14.627222061157227\n",
      "Step 2/500 - Loss: 14.74473762512207\n",
      "Step 3/500 - Loss: 13.444860458374023\n",
      "Step 4/500 - Loss: 13.211654663085938\n",
      "Step 5/500 - Loss: 13.355987548828125\n",
      "Step 6/500 - Loss: 12.792304992675781\n",
      "Step 7/500 - Loss: 13.47061824798584\n",
      "Step 8/500 - Loss: 12.651981353759766\n",
      "Step 9/500 - Loss: 13.394171714782715\n",
      "Step 10/500 - Loss: 13.922688484191895\n",
      "Step 11/500 - Loss: 14.595026969909668\n",
      "Step 12/500 - Loss: 14.697186470031738\n",
      "Step 13/500 - Loss: 13.850632667541504\n",
      "Step 14/500 - Loss: 13.1456298828125\n",
      "Step 15/500 - Loss: 12.221887588500977\n",
      "Step 16/500 - Loss: 13.107438087463379\n",
      "Step 17/500 - Loss: 14.181408882141113\n",
      "Step 18/500 - Loss: 12.432570457458496\n",
      "Step 19/500 - Loss: 12.96461009979248\n",
      "Step 20/500 - Loss: 14.853802680969238\n",
      "Step 21/500 - Loss: 13.6891508102417\n",
      "Step 22/500 - Loss: 14.524425506591797\n",
      "Step 23/500 - Loss: 14.052000045776367\n",
      "Step 24/500 - Loss: 12.678755760192871\n",
      "Step 25/500 - Loss: 14.360322952270508\n",
      "Step 26/500 - Loss: 13.136216163635254\n",
      "Step 27/500 - Loss: 13.342872619628906\n",
      "Step 28/500 - Loss: 12.252355575561523\n",
      "Step 29/500 - Loss: 13.487910270690918\n",
      "Step 30/500 - Loss: 14.14539623260498\n",
      "Step 31/500 - Loss: 13.513200759887695\n",
      "Step 32/500 - Loss: 14.302955627441406\n",
      "Step 33/500 - Loss: 12.709760665893555\n",
      "Step 34/500 - Loss: 12.78776741027832\n",
      "Step 35/500 - Loss: 13.299283981323242\n",
      "Step 36/500 - Loss: 13.701624870300293\n",
      "Step 37/500 - Loss: 14.820992469787598\n",
      "Step 38/500 - Loss: 12.482711791992188\n",
      "Step 39/500 - Loss: 14.493864059448242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[98], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m     loss_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(loss, \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m loss\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 12\u001B[0m \u001B[43mloss_total\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_steps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss_total\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mhasattr\u001B[39m(loss_total,\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39mloss_total\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    647\u001B[0m     )\n\u001B[1;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    825\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    826\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:00:21.990095Z",
     "start_time": "2025-05-14T16:05:11.919332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "num_steps = 500\n",
    "best_loss = float('inf')\n",
    "best_model_state = None   # To hold the best model's state_dict\n",
    "\n",
    "for step in range(num_steps):\n",
    "    supermodel.train()\n",
    "    loss_total = 0.0\n",
    "\n",
    "    for ix in range(1, 1400):  # Or 14001 for full range\n",
    "        loss = supermodel(codes, ix)\n",
    "        loss_total += loss if isinstance(loss, float) else loss.sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_scalar = loss_total.item() if hasattr(loss_total, 'item') else float(loss_total)\n",
    "    print(f\"Step {step+1}/{num_steps} - Loss: {loss_scalar}\")\n",
    "\n",
    "    # Checkpoint the best model\n",
    "    if loss_scalar < best_loss:\n",
    "        best_loss = loss_scalar\n",
    "        best_model_state = copy.deepcopy(supermodel.state_dict())\n",
    "        # Optionally, save to disk\n",
    "        torch.save(best_model_state, \"best_supermodel.pt\")\n",
    "        print(f\"New best model found at step {step+1} with loss {best_loss}\")\n",
    "\n",
    "# After training, you can restore the best model:\n",
    "# supermodel.load_state_dict(best_model_state)"
   ],
   "id": "87e1fccaec717cf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/500 - Loss: 26.40045738220215\n",
      "New best model found at step 1 with loss 26.40045738220215\n",
      "Step 2/500 - Loss: 26.350906372070312\n",
      "New best model found at step 2 with loss 26.350906372070312\n",
      "Step 3/500 - Loss: 26.426298141479492\n",
      "Step 4/500 - Loss: 26.94700813293457\n",
      "Step 5/500 - Loss: 27.481698989868164\n",
      "Step 6/500 - Loss: 26.522672653198242\n",
      "Step 7/500 - Loss: 25.663589477539062\n",
      "New best model found at step 7 with loss 25.663589477539062\n",
      "Step 8/500 - Loss: 27.199718475341797\n",
      "Step 9/500 - Loss: 29.32910919189453\n",
      "Step 10/500 - Loss: 24.1258602142334\n",
      "New best model found at step 10 with loss 24.1258602142334\n",
      "Step 11/500 - Loss: 29.485454559326172\n",
      "Step 12/500 - Loss: 26.24025535583496\n",
      "Step 13/500 - Loss: 27.39120864868164\n",
      "Step 14/500 - Loss: 26.175060272216797\n",
      "Step 15/500 - Loss: 27.111841201782227\n",
      "Step 16/500 - Loss: 25.47618865966797\n",
      "Step 17/500 - Loss: 26.4117374420166\n",
      "Step 18/500 - Loss: 27.417335510253906\n",
      "Step 19/500 - Loss: 27.10292625427246\n",
      "Step 20/500 - Loss: 28.680923461914062\n",
      "Step 21/500 - Loss: 26.7054500579834\n",
      "Step 22/500 - Loss: 27.32565689086914\n",
      "Step 23/500 - Loss: 27.592058181762695\n",
      "Step 24/500 - Loss: 28.837217330932617\n",
      "Step 25/500 - Loss: 28.398387908935547\n",
      "Step 26/500 - Loss: 26.273969650268555\n",
      "Step 27/500 - Loss: 26.344566345214844\n",
      "Step 28/500 - Loss: 28.744590759277344\n",
      "Step 29/500 - Loss: 27.89914321899414\n",
      "Step 30/500 - Loss: 29.206626892089844\n",
      "Step 31/500 - Loss: 27.452070236206055\n",
      "Step 32/500 - Loss: 27.22666358947754\n",
      "Step 33/500 - Loss: 27.46095085144043\n",
      "Step 34/500 - Loss: 28.182331085205078\n",
      "Step 35/500 - Loss: 27.770183563232422\n",
      "Step 36/500 - Loss: 26.896223068237305\n",
      "Step 37/500 - Loss: 28.946569442749023\n",
      "Step 38/500 - Loss: 27.939781188964844\n",
      "Step 39/500 - Loss: 26.571720123291016\n",
      "Step 40/500 - Loss: 27.62511444091797\n",
      "Step 41/500 - Loss: 28.19633674621582\n",
      "Step 42/500 - Loss: 26.346506118774414\n",
      "Step 43/500 - Loss: 26.03204345703125\n",
      "Step 44/500 - Loss: 25.66971778869629\n",
      "Step 45/500 - Loss: 25.447689056396484\n",
      "Step 46/500 - Loss: 27.120086669921875\n",
      "Step 47/500 - Loss: 25.430883407592773\n",
      "Step 48/500 - Loss: 27.256277084350586\n",
      "Step 49/500 - Loss: 28.922971725463867\n",
      "Step 50/500 - Loss: 27.069076538085938\n",
      "Step 51/500 - Loss: 29.472341537475586\n",
      "Step 52/500 - Loss: 26.634572982788086\n",
      "Step 53/500 - Loss: 27.44393539428711\n",
      "Step 54/500 - Loss: 27.514680862426758\n",
      "Step 55/500 - Loss: 30.555706024169922\n",
      "Step 56/500 - Loss: 28.027057647705078\n",
      "Step 57/500 - Loss: 27.74334144592285\n",
      "Step 58/500 - Loss: 27.602806091308594\n",
      "Step 59/500 - Loss: 26.33879852294922\n",
      "Step 60/500 - Loss: 28.330669403076172\n",
      "Step 61/500 - Loss: 28.490833282470703\n",
      "Step 62/500 - Loss: 25.59585189819336\n",
      "Step 63/500 - Loss: 26.50787925720215\n",
      "Step 64/500 - Loss: 26.788150787353516\n",
      "Step 65/500 - Loss: 28.175025939941406\n",
      "Step 66/500 - Loss: 27.996082305908203\n",
      "Step 67/500 - Loss: 27.91519546508789\n",
      "Step 68/500 - Loss: 27.48682975769043\n",
      "Step 69/500 - Loss: 26.753145217895508\n",
      "Step 70/500 - Loss: 27.50535774230957\n",
      "Step 71/500 - Loss: 25.980358123779297\n",
      "Step 72/500 - Loss: 26.980871200561523\n",
      "Step 73/500 - Loss: 28.759958267211914\n",
      "Step 74/500 - Loss: 27.209365844726562\n",
      "Step 75/500 - Loss: 26.37826919555664\n",
      "Step 76/500 - Loss: 27.3072452545166\n",
      "Step 77/500 - Loss: 27.263479232788086\n",
      "Step 78/500 - Loss: 29.204349517822266\n",
      "Step 79/500 - Loss: 28.00227928161621\n",
      "Step 80/500 - Loss: 28.856548309326172\n",
      "Step 81/500 - Loss: 26.268831253051758\n",
      "Step 82/500 - Loss: 26.187999725341797\n",
      "Step 83/500 - Loss: 26.782855987548828\n",
      "Step 84/500 - Loss: 28.377079010009766\n",
      "Step 85/500 - Loss: 25.362159729003906\n",
      "Step 86/500 - Loss: 27.597517013549805\n",
      "Step 87/500 - Loss: 29.934246063232422\n",
      "Step 88/500 - Loss: 28.462495803833008\n",
      "Step 89/500 - Loss: 27.437633514404297\n",
      "Step 90/500 - Loss: 27.99435043334961\n",
      "Step 91/500 - Loss: 27.244720458984375\n",
      "Step 92/500 - Loss: 27.968055725097656\n",
      "Step 93/500 - Loss: 27.80742835998535\n",
      "Step 94/500 - Loss: 28.25379180908203\n",
      "Step 95/500 - Loss: 26.974763870239258\n",
      "Step 96/500 - Loss: 29.0268497467041\n",
      "Step 97/500 - Loss: 27.825870513916016\n",
      "Step 98/500 - Loss: 25.818931579589844\n",
      "Step 99/500 - Loss: 28.807344436645508\n",
      "Step 100/500 - Loss: 28.36123275756836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[101], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m     loss_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(loss, \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m loss\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m     15\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 16\u001B[0m \u001B[43mloss_total\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     19\u001B[0m loss_scalar \u001B[38;5;241m=\u001B[39m loss_total\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(loss_total, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(loss_total)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    647\u001B[0m     )\n\u001B[1;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\mtmt\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    825\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    826\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:04:01.624620Z",
     "start_time": "2025-05-14T17:04:01.608611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# If you saved best_model_state as a file\n",
    "supermodel.load_state_dict(torch.load(\"best_supermodel.pt\"))\n",
    "\n",
    "# Now retrieve pit and dur (detach and move to cpu if needed)\n",
    "pit = supermodel.pit.detach().cpu()\n",
    "dur = supermodel.dur.detach().cpu()\n",
    "\n",
    "print(\"pit shape:\", pit.shape)\n",
    "print(\"dur shape:\", dur.shape)"
   ],
   "id": "3729213241c33bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pit shape: torch.Size([1, 14550, 1])\n",
      "dur shape: torch.Size([1, 14550, 1])\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:04:03.284531Z",
     "start_time": "2025-05-14T17:04:03.268902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantized_pit = torch.round(pit).clamp(21, 100)  # (1, N, 1)\n",
    "clamped_dur = dur.clamp(torch.max(torch.exp(mean_log_dur_tensor)/8, min_dur_tensor).item(),torch.min(torch.exp(mean_log_dur_tensor)*128, torch.tensor(32, device=device)).item())  # (1, N, 1)"
   ],
   "id": "8f4f0d5b90426e8f",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:04:25.494751Z",
     "start_time": "2025-05-14T17:04:25.479124Z"
    }
   },
   "cell_type": "code",
   "source": "quantized_pit.shape",
   "id": "2e2b30995e2230df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14550, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:08:53.535491Z",
     "start_time": "2025-05-14T17:08:53.519862Z"
    }
   },
   "cell_type": "code",
   "source": "codes.shape",
   "id": "cea329696ddacaa8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14550, 6)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:09:37.920789Z",
     "start_time": "2025-05-14T17:09:37.889536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantized_pit_np = quantized_pit.squeeze().cpu().numpy()\n",
    "clamped_dur_np = clamped_dur.squeeze().cpu().numpy()\n",
    "\n",
    "# Replace in codes array\n",
    "codes[:, 3] = quantized_pit_np\n",
    "codes[:, 4] = clamped_dur_np\n"
   ],
   "id": "ec7f23883836c010",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:12:29.178594Z",
     "start_time": "2025-05-14T17:12:29.162586Z"
    }
   },
   "cell_type": "code",
   "source": "codes[:1400, :]",
   "id": "54351ccffec4b5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,  21,   9,   0],\n",
       "       [  1,   0,   0,  71,   2,   1],\n",
       "       [  1,   0,   0,  21,   2,   6],\n",
       "       ...,\n",
       "       [  3, 110,   1,  27,  13,  29],\n",
       "       [  3, 110,   1,  68,   2,  29],\n",
       "       [  3, 110,   1,  90,  12,  29]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:11:39.825389Z",
     "start_time": "2025-05-14T17:11:39.809762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import representation\n",
    "import muspy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "encoding_path = \"../data/sod/processed/notes/encoding.json\"\n",
    "\n",
    "\n",
    "# Set the output base to your Desktop\n",
    "desktop = os.path.expanduser(\"~/Desktop\")\n",
    "out_base = os.path.join(desktop, \"somethingnew\")\n",
    "\n",
    "# Load encoding\n",
    "encoding = representation.load_encoding(encoding_path)"
   ],
   "id": "e87aad4a2ddb3b74",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T17:12:43.155040Z",
     "start_time": "2025-05-14T17:12:43.139413Z"
    }
   },
   "cell_type": "code",
   "source": "music_decoded = representation.decode(codes[:1400,:], encoding)",
   "id": "315196186c7fc422",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T17:13:04.170817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save as MIDI\n",
    "music_decoded.write(f\"{out_base}.mid\")\n",
    "\n",
    "# Save as MusPy JSON\n",
    "music_decoded.save(f\"{out_base}.json\")\n",
    "\n",
    "# Save as piano roll PNG\n",
    "music_decoded.show_pianoroll(track_label=\"program\")\n",
    "plt.savefig(f\"{out_base}.png\")\n",
    "plt.close()"
   ],
   "id": "3109466beefc7803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ee3b32033874309c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
